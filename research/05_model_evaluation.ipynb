{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd217520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Data_science\\\\Projects\\\\kidney_disease_classification\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9f5fb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c14af82a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Data_science\\\\Projects\\\\kidney_disease_classification'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class EvaluationConfig:\n",
    "    \"\"\"\n",
    "    Configuration for the model evaluation stage.\n",
    "    \"\"\"\n",
    "    path_of_model: Path  # Path to the trained model to be evaluated\n",
    "    validation_data: Path  # Path to the validation dataset\n",
    "    mlflow_uri: str  # MLflow tracking server URI\n",
    "    all_params: dict  # A dictionary containing all project parameters for logging\n",
    "    params_image_size: list  # Image dimensions for evaluation\n",
    "    params_batch_size: int  # Batch size for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from KidneyCNN.constants import *\n",
    "from KidneyCNN.utils.common import (read_yaml, create_directories, save_json)\n",
    "\n",
    "\n",
    "# This class manages the configuration settings for the entire project pipeline.\n",
    "# It reads settings from 'config.yaml' and 'params.yaml' and provides them\n",
    "# to each pipeline stage in a structured format.\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "        \"\"\"\n",
    "        Initializes the ConfigurationManager by loading configuration and\n",
    "        parameters from YAML files.\n",
    "        \"\"\"\n",
    "        # Load the main configuration file\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        # Load the model hyperparameters\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        # Create the root artifacts directory if it doesn't exist\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    \n",
    "    def get_data_ingestion_config(self) -> DataIngestionConfig:\n",
    "        \"\"\"\n",
    "        Provides the configuration for the data ingestion stage.\n",
    "        \"\"\"\n",
    "        # Get data ingestion settings from the main config\n",
    "        config = self.config.data_ingestion\n",
    "\n",
    "        # Create the data ingestion root directory\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        # Create a DataIngestionConfig object with the relevant settings\n",
    "        data_ingestion_config = DataIngestionConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            source_URL=config.source_URL,\n",
    "            local_data_file=config.local_data_file,\n",
    "            unzip_dir=config.unzip_dir \n",
    "        )\n",
    "\n",
    "        return data_ingestion_config\n",
    "    \n",
    "\n",
    "    def get_prepear_base_model_config(self) -> PrepareBaseModelConfig:\n",
    "        \"\"\"\n",
    "        Provides the configuration for the base model preparation stage.\n",
    "        \"\"\"\n",
    "        # Get settings from the main config\n",
    "        config = self.config.prepare_base_model\n",
    "\n",
    "        # Create the directory for the base model\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        # Create a PrepareBaseModelConfig object with all necessary parameters\n",
    "        prepare_base_model_config = PrepareBaseModelConfig(\n",
    "            root_dir= Path(config.root_dir),\n",
    "            base_model_path= Path(config.base_model_path),\n",
    "            updated_base_model_path= Path(config.updated_base_model_path),\n",
    "            params_image_size= self.params.IMAGE_SIZE,  # Input image dimensions\n",
    "            params_learning_rate= self.params.LEARNING_RATE, # Learning rate for the optimizer\n",
    "            params_include_top= self.params.INCLUDE_TOP, # Whether to include the final dense layer\n",
    "            params_weights= self.params.WEIGHTS, # Pre-trained weights for the model\n",
    "            params_classes= self.params.CLASSES # The number of output classes, crucial for model architecture\n",
    "        )\n",
    "\n",
    "        return prepare_base_model_config\n",
    "    \n",
    "\n",
    "    def get_traning_config(self) -> TrainingConfig:\n",
    "        \"\"\"\n",
    "        Provides the configuration for the model training stage.\n",
    "        \"\"\"\n",
    "        # Get settings from main config and params files\n",
    "        traning = self.config.training\n",
    "        prepare_base_model = self.config.prepare_base_model\n",
    "        params = self.params\n",
    "        \n",
    "        # Define the path to the training data directory\n",
    "        training_data = os.path.join(self.config.data_ingestion.unzip_dir, \"chest_CT_scan_data\", \"train\")\n",
    "        # Define the path to the validation data directory\n",
    "        validation_data = os.path.join(self.config.data_ingestion.unzip_dir, \"chest_CT_scan_data\", \"valid\")\n",
    "        \n",
    "        # Create the training artifacts directory\n",
    "        create_directories([Path(traning.root_dir)])\n",
    "\n",
    "        # Instantiate the callbacks here using parameters from params.yaml\n",
    "        callbacks = {\n",
    "            \"early_stopping\": EarlyStopping(\n",
    "                monitor=params.CALLBACKS.early_stopping.monitor,\n",
    "                patience=params.CALLBACKS.early_stopping.patience,\n",
    "                verbose=params.CALLBACKS.early_stopping.verbose\n",
    "            ),\n",
    "            \"model_checkpoint\": ModelCheckpoint(\n",
    "                filepath=Path(traning.trained_model_path),\n",
    "                save_best_only=params.CALLBACKS.model_checkpoint.save_best_only,\n",
    "                monitor=params.CALLBACKS.model_checkpoint.monitor\n",
    "            ),\n",
    "            \"reduce_lr_on_plateau\": ReduceLROnPlateau(\n",
    "                monitor=params.CALLBACKS.reduce_lr_on_plateau.monitor,\n",
    "                factor=params.CALLBACKS.reduce_lr_on_plateau.factor,\n",
    "                patience=params.CALLBACKS.reduce_lr_on_plateau.patience\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        # Create a TrainingConfig object with all necessary parameters\n",
    "        traning_config = TrainingConfig(\n",
    "            root_dir=Path(traning.root_dir),\n",
    "            trained_model_path=Path(traning.trained_model_path),\n",
    "            updated_base_model_path=Path(prepare_base_model.updated_base_model_path),\n",
    "            training_data=Path(training_data),\n",
    "            validation_data=Path(validation_data),\n",
    "            params_epochs=params.EPOCHS, # Number of training epochs\n",
    "            params_batch_size=params.BATCH_SIZE, # Batch size for training\n",
    "            params_is_augmentation=params.AUGMENTATION,  # Enable/disable data augmentation\n",
    "            params_image_size=params.IMAGE_SIZE,  # Set the input image size\n",
    "            params_learning_rate=params.LEARNING_RATE,  # Define the learning rate\n",
    "            # Pass the correctly instantiated callback objects to the dataclass\n",
    "            params_callbacks=callbacks\n",
    "        )\n",
    "\n",
    "        return traning_config\n",
    "    \n",
    "\n",
    "    def get_evalution_config(self) -> EvaluationConfig:\n",
    "        \"\"\"\n",
    "        Provides the configuration for the model evaluation stage.\n",
    "        \"\"\"\n",
    "        # Define the path to the validation data directory\n",
    "        validation_data = os.path.join(self.config.data_ingestion.unzip_dir, \"chest_CT_scan_data\", \"valid\")\n",
    "        \n",
    "        # Create an EvaluationConfig object with the relevant settings\n",
    "        eval_config = EvaluationConfig(\n",
    "            path_of_model=\"artifacts/training/model.h5\", # Path to the trained model\n",
    "            validation_data=Path(validation_data), # Path to the validation data\n",
    "            mlflow_uri=\"https://dagshub.com/proshanta000/End_to_End_ml_project_chest_CT_scan.mlflow\", # MLflow tracking URI\n",
    "            all_params= self.params, # All model parameters for logging to MLflow\n",
    "            params_image_size= self.params.IMAGE_SIZE, # Image size for evaluation\n",
    "            params_batch_size= self.params.BATCH_SIZE # Batch size for evaluation\n",
    "        )\n",
    "        return eval_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9e6f8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19f43ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e61ffb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a888a22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef90dd43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4d1c17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7998a78a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6aad5cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
